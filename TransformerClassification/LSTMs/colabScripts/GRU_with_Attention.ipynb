{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/My Drive/LSTM/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder GRU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch=1):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        return torch.zeros(1, batch, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder GRU with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout=0.1, max_input_length=470):\n",
    "        super(AttnDecoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_input_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        # self.attention_network = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.out_attn = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # #####################version 1#######################\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # attn_weights = self.dropout(attn_weights)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "        ####################version 2##########################\n",
    "        # embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # _, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        # attn_weights = F.softmax(torch.mm(hidden[0], encoder_outputs.t()), dim=1)\n",
    "        # attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        # attention_res = torch.cat((hidden[0], attn_applied[0]), 1)\n",
    "        # output = F.log_softmax(self.attention_network(attention_res), dim=1)\n",
    "\n",
    "        # return output, hidden, attn_weights\n",
    "        ####################version 3##############################\n",
    "        # embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # _, hidden = self.gru(embedded, hidden)\n",
    "\n",
    "        # attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # attention_res = torch.cat((hidden[0], attn_applied[0]), 1)\n",
    "\n",
    "        # output = F.log_softmax(self.out_attn(attention_res), dim=1)\n",
    "        # return output, hidden, attn_weights\n",
    "        #####################version 4############################\n",
    "        # embedded = self.embedding(input).view(1, 1, -1)\n",
    "\n",
    "        # attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        # attn_weights = self.dropout(attn_weights)\n",
    "        # attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "        #                          encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        # output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        # output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        # output = F.relu(output)\n",
    "        # output, hidden = self.gru(output, hidden)\n",
    "        # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        # return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross - Validation Fold generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from sklearn import model_selection\n",
    "\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence, device):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def split(pairs, lang, device):\n",
    "    train = []\n",
    "    test = []\n",
    "    for label in pairs:\n",
    "        label_tensor = tensorFromSentence(lang, label, device)\n",
    "        iters = pairs[label]\n",
    "        test_index = random.randint(0, len(iters) - 1)\n",
    "        accept_prob = random.random()\n",
    "        for i in range(len(iters)):\n",
    "            if i == test_index and len(iters) != 1 and accept_prob > 0.5:\n",
    "                test.append([iters[i], label_tensor])\n",
    "            else:\n",
    "                train.append([iters[i], label_tensor])\n",
    "    return train, test\n",
    "\n",
    "def kfoldSplit(pairs, lang, device, split=10):\n",
    "    folds = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for label in pairs:\n",
    "        for iter in pairs[label]:\n",
    "            inputs.append(iter)\n",
    "            outputs.append(label)\n",
    "    \n",
    "    skf = model_selection.StratifiedKFold(n_splits=split, shuffle=True)\n",
    "    indices = skf.split(inputs, outputs)\n",
    "\n",
    "    for train_indices, test_indices in indices:\n",
    "        curr_train = []\n",
    "        curr_test = []\n",
    "        for indices in train_indices:\n",
    "            curr_train.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n",
    "        for indices in test_indices:\n",
    "            curr_test.append([inputs[indices], tensorFromSentence(lang,  outputs[indices], device)])\n",
    "        folds.append([curr_train, curr_test])\n",
    "    \n",
    "    return folds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy calculator and result documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, sentence, output_lang, sil0, sil1, max_input_length=470, max_output_length=5):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence\n",
    "        input_length = len(sentence)\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0,0]\n",
    "\n",
    "        decoder_input = torch.tensor([[sil0]], device=device)\n",
    "        decoder_attentions = torch.zeros(max_output_length, max_input_length)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_output_length):\n",
    "            decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_attentions[di] = decoder_attn.data\n",
    "            if topi.item() == sil1:\n",
    "                decoded_words.append('sil1')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di+1]\n",
    "\n",
    "def calculateTrainingAccuracy(encoder, decoder, pairs, output_lang, sil0, sil1, file_name=None, write = True, max_input_length=470, max_output_length=5):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    results = None\n",
    "    if write:\n",
    "        results = open(file_name, 'w')\n",
    "    attention = None\n",
    "    for pair in pairs:\n",
    "        output_words, attention = evaluate(encoder, decoder, pair[0], output_lang, sil0, sil1, max_input_length=max_input_length, max_output_length=max_output_length)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        sent = [output_lang.index2word[i.item()] for i in pair[1]]\n",
    "        true_sentence = ' '.join(sent)\n",
    "        if write:\n",
    "            print('Predicted Sentence: ', output_sentence)\n",
    "            print('True Sentence: ' , true_sentence)\n",
    "            plt.matshow(attention.numpy())\n",
    "            print('Predicted Sentence: ', output_sentence, file=results)\n",
    "            print('True Sentence: ' , true_sentence, file=results)\n",
    "        answer = None\n",
    "        if output_sentence == true_sentence:\n",
    "            correct += 1\n",
    "            answer = \"CORRECT\"\n",
    "        else:\n",
    "            answer = \"INCORRECT\"\n",
    "        total += 1\n",
    "        if write:\n",
    "            print('Result: ', answer, file=results)\n",
    "    if write:\n",
    "        print('Recognition Total: ', str(correct/total), file=results)\n",
    "        results.close()\n",
    "    return correct/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM training methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random \n",
    "import time\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1, max_input_length = 470):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = len(input_tensor)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0,0]\n",
    "\n",
    "    decoder_input = torch.tensor([[sil0]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == sil1:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1, max_input_length=470):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor\n",
    "        input_length = len(input_tensor)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_input_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0,0]\n",
    "\n",
    "        decoder_input = torch.tensor([[sil0]], device=device)\n",
    "\n",
    "        layers, batches, hidden_num = encoder_hidden.size()\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if topi.item() == sil1:\n",
    "                break\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return loss.item() / target_length\n",
    "\n",
    "def trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, output_lang, lr=1e-4, lr_decay=1, lr_drop_epoch=10, l2_penalty = 0, max_input_length=470, max_output_length = 6):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    test_loss_total = 0\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay = l2_penalty)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr, weight_decay = l2_penalty)\n",
    "\n",
    "    best_test_acc = -1\n",
    "    best_encoder = None\n",
    "    best_decoder = None\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    drop = False #False  = piecewise drop. True = gradual drop.\n",
    "\n",
    "    for iter in range(1, epochs + 1):\n",
    "        if drop:\n",
    "            if iter == lr_drop_epoch:\n",
    "                encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n",
    "                decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr * (lr_decay)**(iter), weight_decay = l2_penalty)\n",
    "        else:\n",
    "            encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr * (lr_decay), weight_decay = l2_penalty)\n",
    "            decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr * (lr_decay), weight_decay = l2_penalty)\n",
    "\n",
    "        for pairs in train_set:\n",
    "            input_tensor = pairs[0]\n",
    "            target_tensor = pairs[1]\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                        decoder, encoder_optimizer, decoder_optimizer, criterion, sil0, sil1, max_input_length=max_input_length)\n",
    "            print_loss_total += loss\n",
    "\n",
    "        for pair in test_set:\n",
    "            input_tensor = pair[0]\n",
    "            target_tensor = pair[1]\n",
    "            test_loss_total += testSetLoss(encoder, decoder, input_tensor, target_tensor, criterion, sil0, sil1, max_input_length=max_input_length)\n",
    "\n",
    "        print_loss_avg = print_loss_total / len(train_set)\n",
    "        test_loss_avg = test_loss_total / len(test_set)\n",
    "        print_loss_total = 0\n",
    "        test_loss_total = 0\n",
    "        test_acc = calculateTrainingAccuracy(encoder, decoder, test_set, output_lang, sil0, sil1, write=False, max_input_length=max_input_length, max_output_length=max_output_length)\n",
    "        train_acc = calculateTrainingAccuracy(encoder, decoder, train_set, output_lang, sil0, sil1, write=False, max_input_length=max_input_length, max_output_length=max_output_length)\n",
    "        print('%s (%d %d%%) train loss: %.4f train acc: %.4f test loss: %.4f test acc: %.4f' % (timeSince(start, iter / epochs),\n",
    "                                        iter, iter / epochs * 100, print_loss_avg, train_acc, test_loss_avg, test_acc))\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_encoder = copy.deepcopy(encoder)\n",
    "            best_decoder = copy.deepcopy(decoder)\n",
    "\n",
    "        plot_losses.append(test_loss_avg)\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return best_encoder, best_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main script - uses above files to run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "\n",
    "#########    HYPERPARAMETERS   ############\n",
    "random.seed(42)\n",
    "users = [\"p1\"]\n",
    "file_name = \"p1\"\n",
    "num_features = 0\n",
    "hidden_size = 2400\n",
    "epochs = 60\n",
    "limit_features = False\n",
    "lr = 1e-4\n",
    "lr_decay = 0.95\n",
    "lr_drop = 20\n",
    "num_layers = 1\n",
    "k_fold = False\n",
    "folds = 5\n",
    "expansion_factor = 2\n",
    "l2_penalty = 0\n",
    "dropout = 0\n",
    "###########################################\n",
    "\n",
    "sil0 = 0\n",
    "sil1 = 0\n",
    "\n",
    "def expand(dataset_as_array, factor):\n",
    "    expanded_array = []\n",
    "    for pair in dataset_as_array:\n",
    "        content = pair[0]\n",
    "        label = pair[1]\n",
    "\n",
    "        expanded_pair = [[[],label] for i in range(factor)]\n",
    "        for frame in range(len(content)):\n",
    "            expanded_pair[frame % factor][0].append(content[frame])\n",
    "        expanded_array.extend(expanded_pair)\n",
    "    return expanded_array\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "eng = Lang(\"english\")\n",
    "pairs = {}\n",
    "max_input_length = 0\n",
    "max_output_length = 0\n",
    "print(\"Reading data from files...\")\n",
    "for user in users:\n",
    "    for file in glob.glob(\"data/\"+user+\"/*.ark\"):\n",
    "        label = \"sil0_\"+file.split(\".\")[1]+\"_sil1\"\n",
    "        label = label.replace(\"_\", \" \")\n",
    "        eng.addSentence(label)\n",
    "\n",
    "        max_output_length = max(max_output_length, len(label.split(\" \")))\n",
    "\n",
    "        sil0 = eng.word2index[\"sil0\"]\n",
    "        sil1 = eng.word2index[\"sil1\"]\n",
    "        content = []\n",
    "        f = open(file)\n",
    "        for x in f:\n",
    "            line = x\n",
    "            if \"[\" in x:\n",
    "                line = x.split(\"[ \")[1]\n",
    "            elif \"]\" in x:\n",
    "                line = x.split(\"]\")[0]\n",
    "            features = []\n",
    "            line = line.strip(\"\\n\").split(\" \")\n",
    "            if limit_features:\n",
    "                line = line[-num_features:]\n",
    "            for f in line:\n",
    "                try:\n",
    "                    features.append(float(f)*1000)\n",
    "                except:\n",
    "                    pass\n",
    "            if len(features) != 0:\n",
    "                num_features = len(features)\n",
    "                content.append(torch.tensor(features, dtype=torch.float, device=device).view(1, 1, -1))\n",
    "        max_input_length = max(max_input_length, math.ceil(len(content)/expansion_factor))\n",
    "        if label in pairs:\n",
    "            temp = pairs[label]\n",
    "            temp.append(content)\n",
    "            pairs[label] = temp\n",
    "        else:\n",
    "            pairs[label] = [content]\n",
    "\n",
    "print(\"Max Input length = \"+str(max_input_length) + \" Max output length = \" + str(max_output_length))\n",
    "\n",
    "for label in pairs:\n",
    "    print(\"Label = \" + label + \" Number of iterations = \" + str(len(pairs[label])))\n",
    "\n",
    "# max_output_length = max_input_length\n",
    "if not k_fold:    \n",
    "    print(\"Splitting data into train and test...\")\n",
    "    train_set, test_set = split(pairs, eng, device)\n",
    "    train_set, test_set = expand(train_set, expansion_factor), expand(test_set, expansion_factor)\n",
    "    encoder = EncoderGRU(num_features, hidden_size, dropout=dropout).to(device)\n",
    "    decoder = AttnDecoderGRU(hidden_size, eng.n_words, dropout=dropout, max_input_length=max_input_length).to(device)\n",
    "    print(\"Split done. Elements in train: %d and elements in test: %d. Starting training...\" % (len(train_set), len(test_set)))\n",
    "    best_encoder, best_decoder = trainIters(encoder, decoder, epochs, train_set, test_set, sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty, max_input_length=max_input_length, max_output_length=max_output_length)\n",
    "    print(\"Training done. Printing stats to file....\")\n",
    "    calculateTrainingAccuracy(best_encoder, best_decoder, test_set, eng, sil0, sil1, 'results/'+file_name+'/results.txt', max_input_length=max_input_length, max_output_length=max_output_length)\n",
    "    print(\"Saving Models\")\n",
    "    torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM.pt\")\n",
    "    torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM.pt\")\n",
    "\n",
    "else:\n",
    "    print(\"Generating folds...\")\n",
    "    trainTestFolds = kfoldSplit(pairs, eng, device, split=folds)\n",
    "    print(\"Fold generation done...\")\n",
    "    fold_num = 1\n",
    "    for curr_fold in trainTestFolds:\n",
    "        encoder = EncoderGRU(num_features, hidden_size, dropout=dropout).to(device)\n",
    "        decoder = AttnDecoderGRU(hidden_size, eng.n_words, dropout=dropout, max_input_length=max_input_length).to(device)\n",
    "        print(\"Starting training on fold %d. %d elements in curr_fold[0] and %d in curr_fold[1]\" % (fold_num, len(curr_fold[0]), len(curr_fold[1])))\n",
    "        best_encoder, best_decoder = trainIters(encoder, decoder, epochs, curr_fold[0], curr_fold[1], sil0, sil1, eng, lr=lr, lr_decay=lr_decay, lr_drop_epoch=lr_drop, l2_penalty=l2_penalty, max_input_length=max_input_length, max_output_length=max_output_length)\n",
    "        print(\"Training done. Saving predictions to file...\")\n",
    "        calculateTrainingAccuracy(best_encoder, best_decoder, curr_fold[1], eng, sil0, sil1, 'results/'+file_name+'/results_fold'+str(fold_num)+'.txt')\n",
    "        print(\"Saving Models\")\n",
    "        torch.save(best_encoder.state_dict(), \"models/\"+file_name+\"/encoderLSTM_fold\"+str(fold_num)+\".pt\")\n",
    "        torch.save(best_decoder.state_dict(), \"models/\"+file_name+\"/decoderLSTM_fold\"+str(fold_num)+\".pt\")\n",
    "        fold_num += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GRU_with_Attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
